{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Import**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gpytorch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tqdm as tqdm\n",
    "from linear_operator import settings\n",
    "\n",
    "import pyro\n",
    "import math\n",
    "import pickle\n",
    "import time\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pyro.distributions as dist\n",
    "from pyro.infer import MCMC, NUTS\n",
    "import arviz as az\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import GP_functions.Loss_function as Loss_function\n",
    "import GP_functions.bound as bound\n",
    "import GP_functions.Estimation as Estimation\n",
    "import GP_functions.Training as Training\n",
    "import GP_functions.Prediction as Prediction\n",
    "import GP_functions.GP_models as GP_models\n",
    "import GP_functions.Tools as Tools\n",
    "import GP_functions.FeatureE as FeatureE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## inputs set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimension_x = 4\n",
    "# inner_dim = 6\n",
    "dimension_y = inner_dim = 8\n",
    "\n",
    "num_train_locations = 12\n",
    "num_test_locations = 6\n",
    "\n",
    "seed_train = 144\n",
    "seed_test = 1301\n",
    "variance = 3.2\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = Tools.generate_MVN_datasets(dimension_x, dimension_y, inner_dim, num_train_locations, num_test_locations, seed_train, seed_test, variance, \n",
    "                                                               sparse_density = 0.3, relationship = 'NInde')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = torch.tensor(X_train, dtype=torch.float32)\n",
    "\n",
    "train_y = torch.tensor(Y_train, dtype=torch.float32)\n",
    "\n",
    "test_x = torch.tensor(X_test, dtype=torch.float32)\n",
    "\n",
    "test_y = torch.tensor(Y_test, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Models**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## local model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_idx = 0\n",
    "\n",
    "input_point = test_y[row_idx,:]\n",
    "local_train_x, local_train_y = Tools.find_k_nearest_neighbors_CPU(input_point, train_x, train_y, k = 100)\n",
    "\n",
    "bounds = bound.get_bounds(local_train_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local GP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_predict_LocalGP(row_idx, train_x, train_y, test_x, test_y, K_num = 100, Device = 'cpu', PCA_trans = 'None'):\n",
    "\n",
    "    input_point = test_y[row_idx,:]\n",
    "    local_train_x, local_train_y = Tools.find_k_nearest_neighbors_CPU(input_point, train_x, train_y, k = K_num)\n",
    "    bounds = bound.get_bounds(local_train_x)\n",
    "    \n",
    "    LocalGP_models, LocalGP_likelihoods = Training.train_one_row_LocalGP(\n",
    "        train_x, train_y, test_y, row_idx, covar_type = 'RBF', k_num=K_num, lr=0.05, num_iterations=5000, patience=10, device=Device\n",
    "    )\n",
    "    \n",
    "    preds = Prediction.full_preds(LocalGP_models, LocalGP_likelihoods, test_x[row_idx,:].unsqueeze(0).to(Device)).cpu().detach().numpy()\n",
    "    if PCA_trans != 'None':\n",
    "        preds = PCA_trans.inverse_transform(preds)\n",
    "\n",
    "    estimated_params, func_loss = Estimation.multi_start_estimation(LocalGP_models, LocalGP_likelihoods, row_idx, test_y, bounds, Estimation.estimate_params_Adam, \n",
    "                                                                num_starts=5, num_iterations=2000, lr=0.01, patience=10, \n",
    "                                                                attraction_threshold=0.1, repulsion_strength=0.1, device=Device)\n",
    "\n",
    "    return preds, estimated_params, func_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = Parallel(n_jobs=-1)(delayed(train_and_predict_LocalGP)(row_idx, train_x, train_y, test_x, test_y) for row_idx in range(test_y.shape[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_test_preds_LocalGP = [item[0] for item in results]  \n",
    "full_test_estimated_params_LocalGP = [item[1] for item in results]\n",
    "full_test_estimated_params_LocalGP_loss = [item[2] for item in results]\n",
    "\n",
    "# full_test_preds_LocalGP = np.vstack(results_preds)\n",
    "# full_test_estimated_params_LocalGP = np.vstack(results_estimated_params)\n",
    "\n",
    "MSE_LocalGP = np.mean((full_test_preds_LocalGP - test_y.numpy()) ** 2)\n",
    "MSE_estimated_LocalGP = np.mean((full_test_estimated_params_LocalGP - test_x.numpy()) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(full_test_estimated_params_LocalGP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(MSE_LocalGP)\n",
    "print(MSE_estimated_LocalGP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.boxplot(full_test_estimated_params_LocalGP_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-task GP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_predict_MGP(row_idx, train_x, train_y, test_x, test_y, K_num = 100, Device = 'cpu', PCA_trans = 'None'):\n",
    "\n",
    "\n",
    "    input_point = test_y[row_idx,:]\n",
    "    local_train_x, local_train_y = Tools.find_k_nearest_neighbors_CPU(input_point, train_x, train_y, k = K_num)\n",
    "    bounds = bound.get_bounds(local_train_x)\n",
    "\n",
    "    MultitaskGP_models, MultitaskGP_likelihoods = Training.train_one_row_MultitaskGP(local_train_x, local_train_y, n_tasks = train_y.shape[1], covar_type = 'RBF', \n",
    "                                                                                     lr=0.05, num_iterations=10000, patience=10, device=Device)\n",
    "\n",
    "    # preds = Prediction.preds_for_one_model(MultitaskGP_models, MultitaskGP_likelihoods, test_x[row_idx,:].unsqueeze(0).to(Device)).squeeze().detach().numpy()\n",
    "    if PCA_trans != 'None':\n",
    "        preds = PCA_trans.inverse_transform(preds)\n",
    "\n",
    "\n",
    "    estimated_params, _ = Estimation.multi_start_estimation(MultitaskGP_models, MultitaskGP_likelihoods, row_idx, test_y, bounds, Estimation.estimate_params_for_one_model_Adam, \n",
    "                                                                    num_starts=5, num_iterations=2000, lr=0.01, patience=20, \n",
    "                                                                    attraction_threshold=0.1, repulsion_strength=0.1, device=Device)\n",
    "\n",
    "    return estimated_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sizes = [4096, 3576, 3076, 2548, 2048, 1524, 1024]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y[:data_sizes[0],:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = Parallel(n_jobs=-1)(delayed(train_and_predict_MGP)(row_idx, train_x, train_y, test_x, test_y) for row_idx in range(test_y.shape[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.vstack(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean((np.vstack(results) - test_x.numpy()) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_test_preds_MGP = [item[0] for item in results]  \n",
    "full_test_estimated_params_MGP = [item[1] for item in results]\n",
    "\n",
    "\n",
    "MSE_MGP = np.mean((full_test_preds_MGP - test_y.numpy()) ** 2)\n",
    "MSE_estimated_MGP = np.mean((full_test_estimated_params_MGP - test_x.numpy()) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(MSE_MGP)\n",
    "print(MSE_estimated_MGP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_test_estimated_params_MGP_loss = [item[2] for item in results]\n",
    "plt.boxplot(full_test_estimated_params_MGP_loss)\n",
    "plt.ylim(-0.001,0.021)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DKL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NN + Local GP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_predict_NNLocalGP(row_idx, train_x, train_y, test_x, test_y, K_num = 100, Device = 'cpu', PCA_trans = 'None'):\n",
    "\n",
    "    input_point = test_y[row_idx,:]\n",
    "    local_train_x, local_train_y = Tools.find_k_nearest_neighbors_CPU(input_point, train_x, train_y, k = K_num)\n",
    "    bounds = bound.get_bounds(local_train_x)\n",
    "    \n",
    "    NNLocalGP_models, NNLocalGP_likelihoods = Training.train_one_row_NNLocalGP_Parallel(train_x, train_y, test_y, row_idx, \n",
    "                                                                                        FeatureE.FeatureExtractor_6, \n",
    "                                                                                        covar_type = 'RBF', k_num = 100, lr=0.05, \n",
    "                                                                                        num_iterations=5000, patience=10, device=Device)\n",
    "    \n",
    "    preds = Prediction.full_preds(NNLocalGP_models, NNLocalGP_likelihoods, test_x[row_idx,:].unsqueeze(0).to(Device)).cpu().detach().numpy()\n",
    "    if PCA_trans != 'None':\n",
    "        preds = PCA_trans.inverse_transform(preds)\n",
    "\n",
    "    estimated_params, func_loss = Estimation.multi_start_estimation(NNLocalGP_models, NNLocalGP_likelihoods, row_idx, test_y, bounds, Estimation.estimate_params_Adam, \n",
    "                                                                    num_starts=5, num_iterations=2000, lr=0.01, patience=10, \n",
    "                                                                    attraction_threshold=0.1, repulsion_strength=0.1, device=Device)\n",
    "\n",
    "    return preds, estimated_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = Parallel(n_jobs=-1)(delayed(train_and_predict_NNLocalGP)(row_idx, train_x, train_y, test_x, test_y) for row_idx in range(test_y.shape[0]))\n",
    "\n",
    "\n",
    "# results_preds = []\n",
    "# results_estimated_params = []\n",
    "# for row_idx in range(test_y.shape[0]):\n",
    "#     preds, estimated_params = train_and_predict_NNLocalGP(row_idx, train_x, train_y, test_x, test_y)\n",
    "#     results_preds.append(preds)\n",
    "#     results_estimated_params.append(estimated_params)\n",
    "#     # results.append((preds, estimated_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_test_preds_NNLocalGP = [item[0] for item in results]  \n",
    "full_test_estimated_params_NNLocalGP = [item[1] for item in results]\n",
    "\n",
    "\n",
    "# full_test_preds_NNLocalGP = np.vstack(results_preds)\n",
    "# full_test_estimated_params_NNLocalGP = np.vstack(results_estimated_params)\n",
    "\n",
    "MSE_NNLocalGP = np.mean((full_test_preds_NNLocalGP - test_y.numpy()) ** 2)\n",
    "MSE_estimated_NNLocalGP = np.mean((full_test_estimated_params_NNLocalGP - test_x.numpy()) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(MSE_NNLocalGP)\n",
    "print(MSE_estimated_NNLocalGP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NN + Multi-task GP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NNMultitaskGP_models, NNMultitaskGP_likelihoods = Training.train_one_row_NNMultitaskGP(local_train_x, local_train_y, n_tasks = train_y.shape[1], \n",
    "                                                                                       feature_extractor_class = FeatureE.FeatureExtractor_1, lr=0.05, num_iterations=5000, patience=10, \n",
    "                                                                                       device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_predict_NNMGP(row_idx, train_x, train_y, test_x, test_y, K_num = 100, Device = 'cpu', PCA_trans = 'None'):\n",
    "\n",
    "\n",
    "    input_point = test_y[row_idx,:]\n",
    "    local_train_x, local_train_y = Tools.find_k_nearest_neighbors_CPU(input_point, train_x, train_y, k = K_num)\n",
    "    bounds = bound.get_bounds(local_train_x)\n",
    "\n",
    "    NNMultitaskGP_models, NNMultitaskGP_likelihoods = Training.train_one_row_NNMultitaskGP(local_train_x, local_train_y, n_tasks = train_y.shape[1], \n",
    "                                                                                           feature_extractor_class = FeatureE.FeatureExtractor_6, \n",
    "                                                                                           covar_type = 'Matern3/2', lr=0.025, num_iterations=5000, patience=20, device = Device)\n",
    "\n",
    "    preds = Prediction.preds_for_one_model(NNMultitaskGP_models, NNMultitaskGP_likelihoods, test_x[row_idx,:].unsqueeze(0).to(Device)).squeeze().detach().numpy()\n",
    "    if PCA_trans != 'None':\n",
    "        preds = PCA_trans.inverse_transform(preds)\n",
    "\n",
    "\n",
    "    estimated_params, func_loss = Estimation.multi_start_estimation(NNMultitaskGP_models, NNMultitaskGP_likelihoods, row_idx, test_y, bounds, Estimation.estimate_params_for_one_model_Adam, \n",
    "                                                                    num_starts=5, num_iterations=2000, lr=0.01, patience=10, \n",
    "                                                                    attraction_threshold=0.1, repulsion_strength=0.1, device=Device)\n",
    "\n",
    "    return preds, estimated_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = Parallel(n_jobs=-1)(delayed(train_and_predict_NNMGP)(row_idx, train_x, train_y, test_x, test_y) for row_idx in range(test_y.shape[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_test_preds_NNMGP = [item[0] for item in results]  \n",
    "full_test_estimated_params_NNMGP = [item[1] for item in results]\n",
    "\n",
    "\n",
    "MSE_NNMGP = np.mean((full_test_preds_NNMGP - test_y.numpy()) ** 2)\n",
    "MSE_estimated_NNMGP = np.mean((full_test_estimated_params_NNMGP - test_x.numpy()) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(MSE_NNMGP)\n",
    "print(MSE_estimated_NNMGP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Device = 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "inducing_points = train_x[:500, :].to(Device)\n",
    "VGP_models_500, VGP_likelihoods_500 = Training.train_full_VGP(train_x, train_y, inducing_points, lr=0.05, device=Device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_preds = Prediction.full_preds_for_VGP(VGP_models_500, VGP_likelihoods_500, train_x)\n",
    "# print('MSE of Train data in VGP_models_200: {}'.format(torch.mean((training_preds - train_y) ** 2).item()))\n",
    "test_preds = Prediction.full_preds_for_VGP(VGP_models_500, VGP_likelihoods_500, test_x)\n",
    "print('MSE of Test data in VGP_models_200: {}'.format(torch.mean((test_preds - test_y) ** 2).item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_predict_VGP(VGP_models_500, VGP_likelihoods_500, row_idx, train_x, train_y, test_y, K_num = 100, Device = 'cpu'):\n",
    "\n",
    "    input_point = test_y[row_idx,:]\n",
    "    local_train_x, local_train_y = Tools.find_k_nearest_neighbors_CPU(input_point, train_x, train_y, k = K_num)\n",
    "    bounds = bound.get_bounds(local_train_x)\n",
    "    \n",
    "\n",
    "    estimated_params, func_loss = Estimation.multi_start_estimation(VGP_models_500, VGP_likelihoods_500, row_idx, test_y, bounds, Estimation.estimate_params_Adam_VGP, \n",
    "                                                                    num_starts=5, num_iterations=2000, lr=0.01, patience=10, \n",
    "                                                                    attraction_threshold=0.1, repulsion_strength=0.1, device=Device)\n",
    "\n",
    "    return estimated_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = Parallel(n_jobs=-1)(delayed(train_and_predict_VGP)(VGP_models_500, VGP_likelihoods_500, row_idx, train_x, train_y, test_y) for row_idx in range(test_y.shape[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_test_estimated_params_VGP = np.vstack(results)\n",
    "\n",
    "MSE_estimated_VGP = np.mean((full_test_estimated_params_VGP - test_x.numpy()) ** 2)\n",
    "print(MSE_estimated_VGP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-task VGP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y_sub = train_y[:1024,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (1024) must match the size of tensor b (4096) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m MVGP_models, MVGP_likelihoods \u001b[38;5;241m=\u001b[39m \u001b[43mTraining\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_full_MultitaskVGP\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_y_sub\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                                                                 \u001b[49m\u001b[43mcovar_type\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mRBF\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_latents\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minner_dim\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_inducing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                                                                 \u001b[49m\u001b[43mlr_hyper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.05\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr_variational\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_iterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# training_preds = Prediction.preds_for_one_model(MVGP_models, MVGP_likelihoods, train_x)\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# print('MSE of Train data in MVGP_models_100: {}'.format(torch.mean((training_preds - train_y) ** 2).item()))\u001b[39;00m\n\u001b[1;32m      6\u001b[0m test_preds \u001b[38;5;241m=\u001b[39m Prediction\u001b[38;5;241m.\u001b[39mpreds_for_one_model(MVGP_models, MVGP_likelihoods, test_x)\n",
      "File \u001b[0;32m~/ToyProblem/GP_functions/Training.py:253\u001b[0m, in \u001b[0;36mtrain_full_MultitaskVGP\u001b[0;34m(train_x_data, train_y_data, covar_type, num_latents, num_inducing, lr_hyper, lr_variational, num_iterations, patience, device)\u001b[0m\n\u001b[1;32m    251\u001b[0m hyperparameter_optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    252\u001b[0m output \u001b[38;5;241m=\u001b[39m model(train_x)\n\u001b[0;32m--> 253\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[43mmll\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_y\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    254\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    255\u001b[0m \u001b[38;5;66;03m# iterator.set_postfix(loss=loss.item())\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/GT/lib/python3.10/site-packages/gpytorch/module.py:31\u001b[0m, in \u001b[0;36mModule.__call__\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Tensor, Distribution, LinearOperator]:\n\u001b[0;32m---> 31\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m     33\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [_validate_module_outputs(output) \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m outputs]\n",
      "File \u001b[0;32m~/miniconda3/envs/GT/lib/python3.10/site-packages/gpytorch/mlls/variational_elbo.py:77\u001b[0m, in \u001b[0;36mVariationalELBO.forward\u001b[0;34m(self, variational_dist_f, target, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, variational_dist_f, target, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     64\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03m    Computes the Variational ELBO given :math:`q(\\mathbf f)` and :math:`\\mathbf y`.\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;124;03m    Calling this function will call the likelihood's :meth:`~gpytorch.likelihoods.Likelihood.expected_log_prob`\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;124;03m    :return: Variational ELBO. Output shape corresponds to batch shape of the model/input data.\u001b[39;00m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 77\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvariational_dist_f\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/GT/lib/python3.10/site-packages/gpytorch/mlls/_approximate_mll.py:58\u001b[0m, in \u001b[0;36m_ApproximateMarginalLogLikelihood.forward\u001b[0;34m(self, approximate_dist_f, target, **kwargs)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# Get likelihood term and KL term\u001b[39;00m\n\u001b[1;32m     57\u001b[0m num_batch \u001b[38;5;241m=\u001b[39m approximate_dist_f\u001b[38;5;241m.\u001b[39mevent_shape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m---> 58\u001b[0m log_likelihood \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_log_likelihood_term\u001b[49m\u001b[43m(\u001b[49m\u001b[43mapproximate_dist_f\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdiv(num_batch)\n\u001b[1;32m     59\u001b[0m kl_divergence \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mvariational_strategy\u001b[38;5;241m.\u001b[39mkl_divergence()\u001b[38;5;241m.\u001b[39mdiv(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_data \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbeta)\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# Add any additional registered loss terms\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/GT/lib/python3.10/site-packages/gpytorch/mlls/variational_elbo.py:61\u001b[0m, in \u001b[0;36mVariationalELBO._log_likelihood_term\u001b[0;34m(self, variational_dist_f, target, **kwargs)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_log_likelihood_term\u001b[39m(\u001b[38;5;28mself\u001b[39m, variational_dist_f, target, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 61\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlikelihood\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpected_log_prob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvariational_dist_f\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/GT/lib/python3.10/site-packages/gpytorch/likelihoods/gaussian_likelihood.py:64\u001b[0m, in \u001b[0;36m_GaussianLikelihoodBase.expected_log_prob\u001b[0;34m(self, target, input, *params, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m     target \u001b[38;5;241m=\u001b[39m settings\u001b[38;5;241m.\u001b[39mobservation_nan_policy\u001b[38;5;241m.\u001b[39m_fill_tensor(target)\n\u001b[1;32m     63\u001b[0m mean, variance \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mmean, \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mvariance\n\u001b[0;32m---> 64\u001b[0m res \u001b[38;5;241m=\u001b[39m ((\u001b[43mtarget\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmean\u001b[49m)\u001b[38;5;241m.\u001b[39msquare() \u001b[38;5;241m+\u001b[39m variance) \u001b[38;5;241m/\u001b[39m noise \u001b[38;5;241m+\u001b[39m noise\u001b[38;5;241m.\u001b[39mlog() \u001b[38;5;241m+\u001b[39m math\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m math\u001b[38;5;241m.\u001b[39mpi)\n\u001b[1;32m     65\u001b[0m res \u001b[38;5;241m=\u001b[39m res\u001b[38;5;241m.\u001b[39mmul(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.5\u001b[39m)\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m nan_policy \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfill\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (1024) must match the size of tensor b (4096) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "MVGP_models, MVGP_likelihoods = Training.train_full_MultitaskVGP(train_x, train_y_sub, \n",
    "                                                                 covar_type = 'RBF', num_latents = inner_dim + 1, num_inducing=100, \n",
    "                                                                 lr_hyper=0.05, lr_variational=0.1, num_iterations=5000, patience=20, device=Device)\n",
    "# training_preds = Prediction.preds_for_one_model(MVGP_models, MVGP_likelihoods, train_x)\n",
    "# print('MSE of Train data in MVGP_models_100: {}'.format(torch.mean((training_preds - train_y) ** 2).item()))\n",
    "test_preds = Prediction.preds_for_one_model(MVGP_models, MVGP_likelihoods, test_x)\n",
    "print('MSE of Test data in MVGP_models_100: {}'.format(torch.mean((test_preds - test_y) ** 2).item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_predict_MVGP(MVGP_models, MVGP_likelihoods, row_idx, train_x, train_y, test_y, K_num = 100, Device = 'cpu'):\n",
    "\n",
    "    input_point = test_y[row_idx,:]\n",
    "    local_train_x, local_train_y = Tools.find_k_nearest_neighbors_CPU(input_point, train_x, train_y, k = K_num)\n",
    "    bounds = bound.get_bounds(local_train_x)\n",
    "    \n",
    "\n",
    "    estimated_params, func_loss = Estimation.multi_start_estimation(MVGP_models, MVGP_likelihoods, row_idx, test_y, bounds, Estimation.estimate_params_for_one_model_Adam, \n",
    "                                                                    num_starts=5, num_iterations=2000, lr=0.01, patience=10, \n",
    "                                                                    attraction_threshold=0.1, repulsion_strength=0.1, device=Device)\n",
    "\n",
    "    return estimated_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = Parallel(n_jobs=-1)(delayed(train_and_predict_MVGP)(MVGP_models, MVGP_likelihoods, row_idx, train_x, train_y, test_y) for row_idx in range(test_y.shape[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_test_estimated_params_MVGP = np.vstack(results)\n",
    "\n",
    "MSE_estimated_MVGP = np.mean((full_test_estimated_params_MVGP - test_x.numpy()) ** 2)\n",
    "print(MSE_estimated_MVGP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DGP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DGP_2 = Training.train_full_DGP_2(train_x, train_y, num_hidden_dgp_dims = dimension_x, inducing_num = 100, num_iterations = 5000, patiences = 50, device=Device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_test_mean = DGP_2.predict(test_x[0,:].unsqueeze(0))[0].detach().numpy()\n",
    "\n",
    "for row_idx in range(1,test_y.shape[0]):\n",
    "    test_mean_tmp = DGP_2.predict(test_x[row_idx,:].unsqueeze(0))[0].detach().numpy()\n",
    "    full_test_mean = np.vstack((full_test_mean, test_mean_tmp))\n",
    "\n",
    "print('MSE of Test data in DGP_2: {}'.format(np.mean((full_test_mean - test_y.detach().numpy()) ** 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_predict_DGP(DGP_2, row_idx, train_x, train_y, test_y, K_num = 100, Device = 'cpu'):\n",
    "\n",
    "    input_point = test_y[row_idx,:]\n",
    "    local_train_x, local_train_y = Tools.find_k_nearest_neighbors_CPU(input_point, train_x, train_y, k = K_num)\n",
    "    bounds = bound.get_bounds(local_train_x)\n",
    "    \n",
    "\n",
    "    estimated_params, func_loss = Estimation.multi_start_estimation_nonliklihood(DGP_2, row_idx, test_y, bounds, Estimation.estimate_params_for_DGP_Adam, \n",
    "                                                                                 num_starts=5, num_iterations=2000, lr=0.01, patience=10, \n",
    "                                                                                 attraction_threshold=0.1, repulsion_strength=0.1, device=Device)\n",
    "\n",
    "    return estimated_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = Parallel(n_jobs=-1)(delayed(train_and_predict_DGP)(DGP_2, row_idx, train_x, train_y, test_y) for row_idx in range(test_y.shape[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_test_estimated_params_DGP = np.vstack(results)\n",
    "\n",
    "MSE_estimated_DGP = np.mean((full_test_estimated_params_DGP - test_x.numpy()) ** 2)\n",
    "print(MSE_estimated_MVGP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NNmodel = Training.train_DNN_4(train_x, train_y, num_iterations = 10000, device=Device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_preds = Prediction.preds_for_DNN(NNmodel, train_x)\n",
    "# Train_loss = torch.mean((train_preds - train_y) ** 2).item()\n",
    "# print('Train_loss:', Train_loss)\n",
    "test_preds  = Prediction.preds_for_DNN(NNmodel, test_x)\n",
    "Test_loss = torch.mean((test_preds - test_y) ** 2).item()\n",
    "print('Test_loss:', Test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_predict_NN(NNmodel, row_idx, train_x, train_y, test_y, K_num = 100, Device = 'cpu'):\n",
    "\n",
    "    input_point = test_y[row_idx,:]\n",
    "    local_train_x, local_train_y = Tools.find_k_nearest_neighbors_CPU(input_point, train_x, train_y, k = K_num)\n",
    "    bounds = bound.get_bounds(local_train_x)\n",
    "    \n",
    "\n",
    "    estimated_params, func_loss = Estimation.multi_start_estimation_nonliklihood(NNmodel, row_idx, test_y, bounds, Estimation.estimate_params_for_NN_Adam, \n",
    "                                                                                 num_starts=5, num_iterations=2000, lr=0.01, patience=10, \n",
    "                                                                                 attraction_threshold=0.1, repulsion_strength=0.1, device=Device)\n",
    "\n",
    "    return estimated_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = Parallel(n_jobs=-1)(delayed(train_and_predict_NN)(NNmodel, row_idx, train_x, train_y, test_y) for row_idx in range(test_y.shape[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_test_estimated_params_NN = np.vstack(results)\n",
    "\n",
    "MSE_estimated_NN = np.mean((full_test_estimated_params_NN - test_x.numpy()) ** 2)\n",
    "print(MSE_estimated_NN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Result**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimension_x_values = [5, 10, 15, 20, 30, 40]\n",
    "\n",
    "k_num_values = [100, 200, 500, 1000]\n",
    "\n",
    "dimension_y_values = [1, 2, 3]\n",
    "\n",
    "inducing_points_values = [100, 200, 500, 1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inner_dim_values = [2]\n",
    "num_train_locations_values = [10,12]\n",
    "dimension_x_values = [2, 4, 8]\n",
    "k_num_values = [100, 200, 500, 1000]\n",
    "dimension_y_values = [2, 4, 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ChangeDimofV/predictions_dict_LocalGP.pkl', 'rb') as file:\n",
    "    predictions_dict_LocalGP = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ChangeDimofV/predictions_dict_VGP.pkl', 'rb') as file:\n",
    "    predictions_dict_VGP = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local GP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_dict_LocalGP = pd.read_pickle(\"ChangeDimofV/predictions_dict_LocalGP.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjusting data structure for the updated visualization request\n",
    "data_to_plot_means = {k_num: [] for k_num in k_num_values}\n",
    "\n",
    "# Calculating means for each combination of dimension_x and k_num\n",
    "for dimension_x in dimension_x_values:\n",
    "    for k_num in k_num_values:\n",
    "        # values = [predictions_dict_LocalGP[(dimension_x, k_num, y_val)] for y_val in dimension_y_values if (dimension_x, k_num, y_val) in predictions_dict_LocalGP]\n",
    "        values = predictions_dict_LocalGP[(dimension_x, k_num, 1)]\n",
    "        # concatenated_values = np.concatenate(values)\n",
    "        data_to_plot_means[k_num].append(np.mean(values))\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 8))\n",
    "colors = ['blue', 'green', 'red', 'purple']\n",
    "for i, (k_num, means) in enumerate(data_to_plot_means.items()):\n",
    "    plt.plot(dimension_x_values, means, 'o-', label=f'k={k_num}', color=colors[i])\n",
    "\n",
    "plt.title('MSE of Predictions for Different K Numbers Across Dimension X Values')\n",
    "plt.xlabel('Dimension X')\n",
    "plt.ylabel('MSE of Predictions')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 3, figsize=(18, 6), sharey=True)\n",
    "\n",
    "# Looping over dimension_y_values to create three plots\n",
    "for ax, dimension_y in zip(axs, dimension_y_values):\n",
    "    data_to_plot_means = {k_num: [] for k_num in k_num_values}\n",
    "    \n",
    "    # Calculating means for each combination of dimension_x and k_num for the given dimension_y\n",
    "    for dimension_x in dimension_x_values:\n",
    "        for k_num in k_num_values:\n",
    "            values = predictions_dict_LocalGP[(dimension_x, k_num, dimension_y)]\n",
    "            data_to_plot_means[k_num].append(np.mean(values))\n",
    "    \n",
    "    # Plotting\n",
    "    for i, (k_num, means) in enumerate(data_to_plot_means.items()):\n",
    "        ax.plot(dimension_x_values, means, 'o-', label=f'k={k_num}', color=colors[i])\n",
    "    \n",
    "    ax.set_title(f'MSE for y={dimension_y}')\n",
    "    ax.set_xlabel('Dimension X')\n",
    "    if ax is axs[0]: # Only add y-label to the first subplot\n",
    "        ax.set_ylabel('MSE of Predictions')\n",
    "    ax.grid(True)\n",
    "\n",
    "axs[0].legend(loc='upper left')\n",
    "plt.suptitle('MSE of Predictions for Different K Numbers Across Dimension X Values')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjusting data structure for the updated visualization request\n",
    "data_to_plot_means = {k_num: [] for k_num in k_num_values}\n",
    "dimension_y_values = sorted(list(set(key[2] for key in predictions_dict_LocalGP.keys())))\n",
    "\n",
    "# Calculating means for each combination of dimension_x and k_num\n",
    "for dimension_y in dimension_y_values:\n",
    "    for k_num in k_num_values:\n",
    "        # values = [predictions_dict_LocalGP[(dimension_x, k_num, y_val)] for y_val in dimension_y_values if (dimension_x, k_num, y_val) in predictions_dict_LocalGP]\n",
    "        values = predictions_dict_LocalGP[(20, k_num, dimension_y)]\n",
    "        # concatenated_values = np.concatenate(values)\n",
    "        data_to_plot_means[k_num].append(np.mean(values))\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 8))\n",
    "colors = ['blue', 'green', 'red', 'purple']\n",
    "for i, (k_num, means) in enumerate(data_to_plot_means.items()):\n",
    "    plt.plot(dimension_y_values, means, 'o-', label=f'k={k_num}', color=colors[i])\n",
    "\n",
    "plt.title('MSE of Predictions for Different K Numbers Across Dimension Y Values')\n",
    "plt.xlabel('Dimension Y')\n",
    "plt.xticks(dimension_y_values)\n",
    "plt.ylabel('MSE of Predictions')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN+LocalGP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_dict_NNLocalGP = pd.read_pickle(\"ChangeDimofV/predictions_dict_NNLocalGP.pkl\")\n",
    "estimated_dict_NNLocalGP = pd.read_pickle(\"ChangeDimofV/estimated_dict_NNLocalGP.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_dict_NNLocalGP = pd.read_pickle(\"ChangeDimofV/predictions_dict_LocalGP_0.pkl\")\n",
    "estimated_dict_NNLocalGP = pd.read_pickle(\"ChangeDimofV/estimated_dict_LocalGP_0.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (inner_dim, num_train_locations, dimension_x, k_num, dimension_y), predictions in estimated_dict_NNLocalGP.items():\n",
    "    print(f\"Parameters: inner_dim = {inner_dim}, num_train_locations={num_train_locations}, dimension_x={dimension_x}, k_num={k_num}, dimension_y={dimension_y}\")\n",
    "    print(f\"Predictions: {predictions}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (inner_dim, num_train_locations, dimension_x, k_num, dimension_y), predictions in estimated_dict_NNLocalGP.items(): \n",
    "    average_prediction = sum(predictions) / len(predictions)\n",
    "\n",
    "    print(f\"Parameters: inner_dim = {inner_dim}, num_train_locations={num_train_locations}, dimension_x={dimension_x}, k_num={k_num}, dimension_y={dimension_y}\")\n",
    "    print(f\"Average Prediction: {average_prediction}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_dict_VGP = pd.read_pickle(\"ChangeDimofV/predictions_dict_VGP.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 3, figsize=(18, 6), sharey=True)\n",
    "colors = ['blue', 'green', 'red', 'purple']\n",
    "# Looping over dimension_y_values to create three plots\n",
    "for ax, dimension_y in zip(axs, dimension_y_values):\n",
    "    data_to_plot_means = {inducing_num: [] for inducing_num in inducing_points_values}\n",
    "    \n",
    "    # Calculating means for each combination of dimension_x and k_num for the given dimension_y\n",
    "    for dimension_x in dimension_x_values:\n",
    "        for inducing_num in inducing_points_values:\n",
    "            values = predictions_dict_VGP[(dimension_x, inducing_num, dimension_y)]\n",
    "            data_to_plot_means[inducing_num].append(np.mean(values))\n",
    "    \n",
    "    # Plotting\n",
    "    for i, (inducing_num, means) in enumerate(data_to_plot_means.items()):\n",
    "        ax.plot(dimension_x_values, means, 'o-', label=f'inducing_num={inducing_num}', color=colors[i])\n",
    "    \n",
    "    ax.set_title(f'MSE for y={dimension_y}')\n",
    "    ax.set_xlabel('Dimension X')\n",
    "    if ax is axs[0]: # Only add y-label to the first subplot\n",
    "        ax.set_ylabel('MSE of Predictions')\n",
    "    ax.grid(True)\n",
    "\n",
    "axs[0].legend(loc='upper left')\n",
    "plt.yticks([0.02,0.05,0.11,0.12,0.14])\n",
    "# ax.set_ylim([-0.00001, 0.12])\n",
    "plt.suptitle('MSE of Predictions for Different Numbers of Inducing point Across Dimension X Values')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
